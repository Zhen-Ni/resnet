* Implementation of ResNet for CIFAR-10 dataset
** Introduction
ResNet is the most famous CNN network for image classification. Its
structure is also widely used for other tasks concerning computer vision.

Resnet was proposed by Kaiming He and his coworkers in the paper [[https://arxiv.org/abs/1512.03385][Deep
Residual Learning for Image Recognition]]. As a famous machine-learning
library, pytorch implemented ResNet in its ~torchvision~
library. Unfortunately, this implementation has some slight difference
with the original paper. For example, the paper introduces Option A
and Option B for Resnet, but the ~torchvision~ library only implemented
the latter one. Thus, it is of vital importance to build a model same
as that in the paper.

Due to limitation of computation resources, only Resnet for CIFAR-10
dataset is implemented and studied here.

** Package Description
This package tries to implement the ResNet on CIFAR-10 dataset
described in Chapter 4.2 in He's paper. Both Option A and Option B are
implemented. In Option A, the resolution of the image is halved when
the number of features is doubled. This can be done by either ignoring
half of the pixels or using a maxpool. Here, both of this approaches
are implemented, with Option A1 (the default) is the former one and
Option A2 is the latter one.

The model and the trainer are organized in different files. The result
of the models can also be plotted. The files of the package are
organized as follows.

| Filename       | Description                                                       |
|----------------+-------------------------------------------------------------------|
| resnet.py      | implementation of resnet and corresponding dataset                |
| trainer.py     | provides a trainer to train and test the model                    |
| show__{}result.py | plot the training and testing error                               |
| main.py        | run this file to train the model, with option A1                  |
| main-2.py      | same as ~main.py~, but trains ResNet model option A2                |
| main-3.py      | same as ~main.py~, but a new dataloader is generated for each epoch |
|                |                                                                   |


** Training Process
Here, the CIFAR-10 dataset is used for traning, with 50k/10k split of
the dataset, which is done by ~torchvision.datasets.CIFAR10~. The mean
values of the three channels of the input dataset are normalized to
0.485, 0.456, 0.406, respectively, and the std values are normalized
to 0.229, 0.224, 0.225. 

The training has a total of 200 epochs. The
learning rate is started at 0.1, and divided by 10 at 100-th epoch and
150-th epoch.

The other settings are consistent with that described in the paper.

** Results


The following results are given by running ~main.py~ and ~main-2.py~,
which give results for Error (A1) and Error (A2) respectively.

/The results are generated by running ~main.py~, ~main-2.py~ and ~main-5.py~./
| Name     | n | Layers | Error (A1) | Error (A2) | Error (B) | Comment                |
|----------+---+--------+------------+------------+-----------+------------------------|
| resnet8  | 1 |      8 |      11.7% |      11.5% |   11.2%   | not presented in paper |
| resnet20 | 3 |     20 |      8.45% |      8.27% |   8.62%   |                        |
| resnet32 | 5 |     32 |      7.17% |       7.8% |   7.86    |                        |

** Discussions
*** Regenerating Datasets for Each Epoch
In the paper, data augmentation is done by randomly cropping and
flipping the images. However, The dataset used for training in ~main.py~
is generated only once, which I thought it might not be valid for data
augmentation. So, ~main-3.py~ is written to get a new dataset with the
same transformation for each epoch. But I soon realized the
transformation is done each time when getting items from the
dataset. Thus, regenerating datasets is actually non-sence, which
should be actually the same as ~main.py~. The results of the two are
listed below.

From the results, we could obtain that although the model structure
and training processe are actually idential, the results show a slight
difference, less than 1%. Slight difference like this should be the
normal behavior of neural networks.

/The results are generated by running ~main.py~ and ~main-3.py~./

| Name     | n | Layers | Error (One dataset) | Error (new dataset for each epoch) |  Comment               |
|----------+---+--------+---------------------+------------------------------------+------------------------|
| resnet8  | 1 |      8 |               11.7% |                              12.4% | not presented in paper |
| resnet20 | 3 |     20 |               8.45% |                              7.91% |                        |
| resnet32 | 5 |     32 |               7.17% |                                    |                        |

*** Effect of Normalized Dataset
In the implementation of ~get_dataloader~, the images are normalized to
~mean = [0.485, 0.456, 0.406]~ and ~std = [0.229, 0.224, 0.225]~. Here, to
show the effect of this proceduce, dataset without this normalization
is trained and tested.

From the results, it can be found that there's *no significant
difference between the two*.

/The results are generated by running ~main.py~ and ~main-4.py~./

| Name     | n | Layers | Error (normalized) | Error (not normalized) | Comment                |
|----------+---+--------+--------------------+------------------------+------------------------|
| resnet8  | 1 |      8 |              11.7% |                  11.8% | not presented in paper |
| resnet20 | 3 |     20 |              8.45% |                  7.92% |                        |
| resnet32 | 5 |     32 |              7.17% |                  7.33% |                        |
| resnet44 | 7 |     44 |                    |                  7.48% |                        |

*** Effect of Shuffling
The training dataset is usually shuffled for each epoch. Here, the
results of training without shuffling the training dataset is also
presented.

/The results are generated by running ~main.py~ and ~main-6.py~./

| Name     | n | Layers | Error (shuffled) | Error (unshuffled) | Comment                |
|----------+---+--------+------------------+--------------------+------------------------|
| resnet8  | 1 |      8 |            11.7% |              12.2% | not presented in paper |
| resnet20 | 3 |     20 |            8.45% |              8.80% |                        |
| resnet32 | 5 |     32 |            7.17% |              8.21% |                        |

